# ğŸš€ Transformation SaaS - Changelog & Architecture

## Vue d'ensemble

Cette mise Ã  jour majeure transforme le prototype en une plateforme SaaS de niveau production avec :

- **Orchestrateur Intelligent** : Routage automatique pour rÃ©duire la latence de 60-80%
- **Multi-Providers LLM** : Support Mistral, OpenAI, Gemini avec switching dynamique
- **Mode RÃ©flexion** : Chain-of-Thought pour des rÃ©ponses approfondies
- **Streaming SSE** : Feedback temps rÃ©el avec Ã©tapes de progression

---

## ğŸ“ Nouveaux Fichiers CrÃ©Ã©s

### Backend

```
backend/src/providers/llm/
â”œâ”€â”€ __init__.py          # Exports du module LLM
â”œâ”€â”€ base_llm.py          # Classe abstraite BaseLLMProvider (Pattern Strategy)
â”œâ”€â”€ mistral_provider.py  # Provider Mistral avec streaming
â”œâ”€â”€ openai_provider.py   # Provider OpenAI (GPT-4o, GPT-3.5)
â”œâ”€â”€ gemini_provider.py   # Provider Google Gemini
â””â”€â”€ factory.py           # LLMProviderFactory avec cache et registry

backend/src/services/
â””â”€â”€ orchestrator.py      # QueryOrchestrator - Routage intelligent
```

### Frontend

```
frontend/src/components/chat/
â””â”€â”€ ProcessingSteps.tsx  # Composants UI pour les Ã©tapes de traitement
```

---

## ğŸ“ Fichiers ModifiÃ©s

### Backend

| Fichier                  | Modifications                                                  |
| ------------------------ | -------------------------------------------------------------- |
| `services/rag_engine.py` | Refactorisation complÃ¨te avec orchestrateur et multi-providers |
| `services/__init__.py`   | Exports de l'orchestrateur et du rate limiter                  |
| `providers/__init__.py`  | Exports des LLM providers                                      |
| `api/routes.py`          | Endpoint `/query/stream` SSE, nouvelles options query          |
| `api/schemas.py`         | Champs `enable_reflection`, `use_rag`, `routing`               |
| `config/settings.py`     | ClÃ©s API OpenAI, Gemini, DeepSeek                              |
| `.env.example`           | Variables pour les providers alternatifs                       |

### Frontend

| Fichier                       | Modifications                                       |
| ----------------------------- | --------------------------------------------------- |
| `stores/preferencesStore.ts`  | `forceRag`, `enableReflection`, `useStreaming`      |
| `types/api.ts`                | Types `RoutingInfo`, `StreamEvent`, `StreamingStep` |
| `hooks/useChat.ts`            | Support des nouvelles options et rÃ©ponses enrichies |
| `app/(console)/chat/page.tsx` | Toggles RÃ©flexion et Documents, nouveaux imports    |

---

## ğŸ”§ Configuration Requise

### Variables d'environnement (optionnelles)

```env
# Alternative LLM Providers
OPENAI_API_KEY=sk-...
GEMINI_API_KEY=AI...
DEEPSEEK_API_KEY=...
DEFAULT_LLM_PROVIDER=mistral
```

### DÃ©pendances Python (optionnelles)

```bash
# Pour OpenAI
pip install openai

# Pour Gemini
pip install google-generativeai
```

---

## ğŸ¯ Architecture de l'Orchestrateur

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    QueryOrchestrator                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  1. Quick Detection (Patterns)                               â”‚
â”‚     â”œâ”€â”€ Greetings â†’ Direct LLM                               â”‚
â”‚     â”œâ”€â”€ Document keywords â†’ RAG                              â”‚
â”‚     â””â”€â”€ Web keywords â†’ Perplexity                           â”‚
â”‚                                                              â”‚
â”‚  2. Smart Routing (LLM-based)                               â”‚
â”‚     â””â”€â”€ mistral-tiny classifie l'intent en JSON             â”‚
â”‚                                                              â”‚
â”‚  3. Intent Types                                             â”‚
â”‚     â”œâ”€â”€ GENERAL â†’ LLM seul (latence minimale)               â”‚
â”‚     â”œâ”€â”€ DOCUMENTS â†’ RAG + LLM                               â”‚
â”‚     â”œâ”€â”€ WEB_SEARCH â†’ Perplexity + LLM                       â”‚
â”‚     â”œâ”€â”€ HYBRID â†’ RAG + Perplexity + LLM                     â”‚
â”‚     â””â”€â”€ GREETING â†’ RÃ©ponse rapide                           â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”Œ Pattern Strategy (Multi-Provider)

```python
# Utilisation basique
from src.providers.llm import get_llm_provider, LLMConfig

# Provider par dÃ©faut (Mistral)
provider = get_llm_provider()
response = await provider.generate(messages)

# Provider spÃ©cifique
config = LLMConfig(model="gpt-4o", temperature=0.5)
openai_provider = get_llm_provider("openai", config)

# Mode rÃ©flexion
response = await provider.generate_with_reflection(messages)
print(response.thought_process)  # PensÃ©es internes
print(response.content)          # RÃ©ponse finale
```

---

## ğŸ“¡ API Endpoints

### POST /api/v1/query

RequÃªte standard avec routage intelligent.

```json
{
  "question": "Quels sont mes projets Python ?",
  "use_rag": true,
  "use_web_search": false,
  "enable_reflection": true
}
```

RÃ©ponse enrichie :

```json
{
  "answer": "D'aprÃ¨s tes projets...",
  "sources": [...],
  "thought_process": "L'utilisateur demande ses projets...",
  "routing": {
    "intent": "documents",
    "use_rag": true,
    "use_web": false,
    "confidence": 0.92,
    "latency_ms": 45
  }
}
```

### POST /api/v1/query/stream

Streaming SSE pour feedback temps rÃ©el.

Events Ã©mis :

- `routing` : DÃ©cision de routage
- `search_start` / `search_complete` : Progression des recherches
- `generation_start` : DÃ©but de gÃ©nÃ©ration
- `chunk` : Morceaux de rÃ©ponse
- `thought` : PensÃ©es internes (mode rÃ©flexion)
- `complete` : Fin avec mÃ©tadonnÃ©es

---

## ğŸ¨ Nouvelles Options UI

### Barre d'options du chat

| Option           | Description      | Effet                     |
| ---------------- | ---------------- | ------------------------- |
| ğŸŒ Recherche web | Force Perplexity | `use_web_search: true`    |
| ğŸ“„ Mes documents | Force le RAG     | `use_rag: true`           |
| ğŸ§  RÃ©flexion     | Chain-of-Thought | `enable_reflection: true` |

---

## ğŸ“ˆ Gains de Performance Attendus

| ScÃ©nario           | Avant | AprÃ¨s    | Gain       |
| ------------------ | ----- | -------- | ---------- |
| Question gÃ©nÃ©rale  | 3-5s  | 0.5-1s   | **70-80%** |
| Salutation         | 3-5s  | 0.2-0.5s | **85-90%** |
| Question documents | 3-5s  | 2-3s     | **30-40%** |
| Question hybride   | 5-8s  | 4-6s     | **20-25%** |

---

## ğŸ”œ Prochaines Ã‰tapes

1. **Tests** : Ã‰crire des tests unitaires pour l'orchestrateur
2. **Migration DB** : Ajouter `reflection_data` aux logs de conversation
3. **Frontend Streaming** : ImplÃ©menter la consommation des SSE
4. **Rate Limiting Mode RÃ©flexion** : Limiter les tokens pour CoT
5. **BYOK (Bring Your Own Key)** : Permettre aux utilisateurs d'utiliser leurs propres clÃ©s API

---

## ğŸ›¡ï¸ Points de Vigilance

- **Hallucinations du Routeur** : Le bouton "Mes documents" permet de forcer le RAG
- **CoÃ»t Mode RÃ©flexion** : Consomme ~2x plus de tokens
- **Feedback Loop** : Le logging est asynchrone (Background Task prÃ©vu)

---

## ğŸ¤– Agent Marketplace v1 (Nouvelle fonctionnalitÃ©)

### Concept

Chaque clÃ© API devient un **agent configurable** avec :

- **ModÃ¨le LLM** personnalisÃ© (Mistral, OpenAI, Deepseek)
- **Prompt systÃ¨me** dÃ©diÃ©
- **Documents RAG** isolÃ©s

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       API Key = Agent                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Configuration Agent (stockÃ©e en DB) :                       â”‚
â”‚  â”œâ”€â”€ model_id: "deepseek-chat" | "gpt-4o" | "mistral-large" â”‚
â”‚  â”œâ”€â”€ system_prompt: "Tu es un expert en..."                 â”‚
â”‚  â”œâ”€â”€ rag_enabled: true/false                                â”‚
â”‚  â””â”€â”€ agent_name: "Mon Assistant"                            â”‚
â”‚                                                              â”‚
â”‚  Isolation Documents :                                       â”‚
â”‚  â””â”€â”€ documents.api_key_id â†’ FK vers api_keys.id             â”‚
â”‚      (ON DELETE CASCADE)                                     â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Nouveaux Fichiers

#### Backend

```
backend/scripts/migrations/
â””â”€â”€ 008_agent_config.sql        # Extension schema api_keys + documents

backend/src/providers/llm/
â””â”€â”€ deepseek_provider.py        # Provider Deepseek (API OpenAI-compatible)

backend/src/api/
â””â”€â”€ routes_agent.py             # GET/PATCH /api/v1/agent/config
```

#### Frontend

```
frontend/src/hooks/
â””â”€â”€ useAgentConfig.ts           # Hooks React Query

frontend/src/components/
â”œâ”€â”€ playground/
â”‚   â””â”€â”€ AgentConfigPanel.tsx    # Panneau de configuration
â””â”€â”€ onboarding/
    â””â”€â”€ OnboardingTour.tsx      # Tour interactif react-joyride
```

### Nouveaux Endpoints API

| Endpoint                         | MÃ©thode | Description                        |
| -------------------------------- | ------- | ---------------------------------- |
| `/api/v1/agent/config`           | GET     | RÃ©cupÃ©rer la config de l'agent     |
| `/api/v1/agent/config`           | PATCH   | Mettre Ã  jour (model, prompt, rag) |
| `/api/v1/agent/available-models` | GET     | Liste des modÃ¨les disponibles      |

### Migration Database

```sql
-- Extension api_keys
ALTER TABLE api_keys ADD COLUMN model_id VARCHAR(100) DEFAULT 'mistral-large-latest';
ALTER TABLE api_keys ADD COLUMN system_prompt TEXT;
ALTER TABLE api_keys ADD COLUMN rag_enabled BOOLEAN DEFAULT TRUE;
ALTER TABLE api_keys ADD COLUMN agent_name VARCHAR(200);

-- Isolation documents par agent
ALTER TABLE documents ADD COLUMN api_key_id UUID REFERENCES api_keys(id) ON DELETE CASCADE;
CREATE INDEX idx_documents_api_key ON documents(api_key_id);

-- validate_api_key retourne maintenant la config agent
```

### Flow Complet

```
1. Utilisateur crÃ©e une clÃ© API (avec agent_config optionnel)
   â†’ POST /api/v1/console/keys { agent_config: { model_id: "deepseek-chat" } }
   â†’ ClÃ© + config stockÃ©es en DB

2. Playground charge la config
   â†’ GET /api/v1/agent/config (auth via Bearer token)
   â†’ Affiche dans AgentConfigPanel

3. Utilisateur modifie la config
   â†’ PATCH /api/v1/agent/config { model_id: "gpt-4o" }
   â†’ Sauvegarde en DB

4. Appel API externe
   â†’ POST /api/v1/query avec X-API-Key
   â†’ Middleware injecte agent_config dans request.state
   â†’ RAG Engine utilise model_id + system_prompt + api_key_id
```

### Onboarding Interactif

- **DÃ©marrage automatique** au premier login (localStorage)
- **6 Ã©tapes** guidÃ©es :
  1. Bienvenue
  2. Gestion des clÃ©s API
  3. CrÃ©ation d'agent
  4. Playground
  5. Documentation
  6. Conclusion
- **Persistance** : `localStorage.rag_onboarding_completed`

### ModÃ¨les LLM SupportÃ©s

| Provider     | ModÃ¨les                                             | Notes                 |
| ------------ | --------------------------------------------------- | --------------------- |
| **Mistral**  | mistral-large-latest, mistral-medium, mistral-small | Par dÃ©faut            |
| **OpenAI**   | gpt-4o, gpt-4o-mini, gpt-4-turbo                    | NÃ©cessite clÃ©         |
| **Deepseek** | deepseek-chat, deepseek-coder, deepseek-reasoner    | API OpenAI-compatible |

---

## ğŸš€ DÃ©ploiement

### 1. Appliquer la migration

```bash
# Via Supabase CLI
supabase migration apply --local

# Ou directement sur Supabase Studio
# Coller le contenu de 008_agent_config.sql
```

### 2. Variables d'environnement

```env
# Nouveau provider
DEEPSEEK_API_KEY=sk-...

# Existants (rappel)
OPENAI_API_KEY=sk-...
GEMINI_API_KEY=AI...
```

### 3. Frontend

```bash
cd frontend
npm install react-joyride @radix-ui/react-switch --legacy-peer-deps
npm run build
```
